{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9857c215a128246d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad9b86-23be-4f55-a4e7-83368a0ec0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_path = './data/annotations_v2/semeval2024_dev_release/subtask2a/train.json'\n",
    "train_images_folder_path = './data/train_images'\n",
    "device = \"cuda:0\" #cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec1c1ba-3eb8-4c22-81d1-a538e66dddbc",
   "metadata": {},
   "source": [
    "### Pretrained Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f94e5e-7253-4123-880d-79ee39ede635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load pre-trained RoBERTa model and tokenizer\n",
    "model_name = 'roberta-base'  # or any other pre-trained model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "roberta_model = RobertaModel.from_pretrained(model_name)\n",
    "roberta_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b08ac0-1d36-4508-92c2-68f3e5f6226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "# xlm_model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "# # xlm_model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# for param in xlm_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# xlm_model.to(device)\n",
    "\n",
    "# # prepare input\n",
    "# text = \"dummy demo ok\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt',truncation=True,max_length=100,padding=True)\n",
    "# encoded_input.to(\"cuda:0\")\n",
    "# # forward pass\n",
    "# output = model(**encoded_input)\n",
    "\n",
    "# features = torch.mean(output.logits, dim=1)\n",
    "# features = features.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6b92a-b2c6-43ee-9612-e7733e0aba50",
   "metadata": {},
   "source": [
    "### Pretrained Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ad915-0589-4283-97a4-629ed4fcccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = torchvision.models.resnet152(weights='DEFAULT')\n",
    "for param in resnet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "resnet_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf3529-4f9d-4e28-899f-94583ddba556",
   "metadata": {},
   "source": [
    "### Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1946f4e-2f4f-4039-a8bc-d29b3e10cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self, train_json_path , train_images_folder_path , device=\"cuda:0\"):\n",
    "        self.train_json_data = self.read_json_data(train_json_path)\n",
    "        self.pretrained_image_model = resnet_model\n",
    "        self.pretrained_text_model = roberta_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_images_folder_path = train_images_folder_path\n",
    "        self.transform = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Lambda(lambda x: x[:3, :, :])  # Keep only the first 3 channels (R, G, B)\n",
    "                         ])\n",
    "        self.max_length = 512\n",
    "        self.device = device\n",
    "        self.all_targets = [\n",
    "                            \"Logos\",\n",
    "                            \"Repetition\",\n",
    "                            \"Obfuscation, Intentional vagueness, Confusion\",\n",
    "                            \"Reasoning\",\n",
    "                            \"Justification\",\n",
    "                            \"Slogans\",\n",
    "                            \"Bandwagon\",\n",
    "                            \"Appeal to authority\",\n",
    "                            \"Flag-waving\",\n",
    "                            \"Appeal to fear/prejudice\",\n",
    "                            \"Simplification\",\n",
    "                            \"Causal Oversimplification\",\n",
    "                            \"Black-and-white Fallacy/Dictatorship\",\n",
    "                            \"Thought-terminating cliché\",\n",
    "                            \"Distraction\",\n",
    "                            \"Misrepresentation of Someone's Position (Straw Man)\",\n",
    "                            \"Presenting Irrelevant Data (Red Herring)\",\n",
    "                            \"Whataboutism\",\n",
    "                            \"Ethos\",\n",
    "                            \"Glittering generalities (Virtue)\",\n",
    "                            \"Ad Hominem\",\n",
    "                            \"Doubt\",\n",
    "                            \"Name calling/Labeling\",\n",
    "                            \"Smears\",\n",
    "                            \"Reductio ad hitlerum\",\n",
    "                            \"Pathos\",\n",
    "                            \"Exaggeration/Minimisation\",\n",
    "                            \"Loaded Language\",\n",
    "                            \"Transfer\",\n",
    "                            \"Appeal to (Strong) Emotions\"\n",
    "                    ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_json_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if(idx in [163 , 1687 , 1769 , 3683]):\n",
    "            idx = 1000\n",
    "            \n",
    "        image_name = self.train_json_data[idx]['image']\n",
    "        text_content = self.train_json_data[idx]['text']\n",
    "        labels = self.train_json_data[idx]['labels']\n",
    "        raw_gt = self.get_ground_truth(labels)\n",
    "        gt_tensor = self.convert_gt_to_tensor(raw_gt)\n",
    "        image_tensor = self.read_image(image_name)\n",
    "        image_features = self.pretrained_image_model(image_tensor.to(self.device)).squeeze()\n",
    "        text_features = self.get_text_tensor(text_content)\n",
    "        return image_features , text_features , gt_tensor\n",
    "\n",
    "    def read_image(self,image_name):\n",
    "        image = Image.open(f\"{self.train_images_folder_path}/{image_name}\")\n",
    "        image_tensor = self.transform(image)\n",
    "        image_tensor = image_tensor.reshape((1,image_tensor.shape[0],image_tensor.shape[1],image_tensor.shape[2]))\n",
    "        return image_tensor\n",
    "\n",
    "    def get_text_tensor(self,text):\n",
    "        tokens = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        tokens.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.pretrained_text_model(**tokens)\n",
    "            \n",
    "        features = outputs.last_hidden_state.mean(dim=1)\n",
    "        return features.squeeze()\n",
    "\n",
    "\n",
    "    def convert_gt_to_tensor(self,gt):\n",
    "        categories = self.all_targets\n",
    "        num_categories = len(categories)\n",
    "        multi_label_ground_truth = [gt]\n",
    "\n",
    "        # Create a tensor for multi-label classification\n",
    "        tensor = np.zeros((len(multi_label_ground_truth), num_categories))\n",
    "        for i, labels in enumerate(multi_label_ground_truth):\n",
    "            indices = [categories.index(label) for label in labels]\n",
    "            tensor[i, indices] = 1\n",
    "\n",
    "        tensor = torch.tensor(tensor)\n",
    "        return tensor.squeeze()\n",
    "        # return tensor\n",
    "\n",
    "    def get_ground_truth(self,labels):\n",
    "        ground_truth = {}\n",
    "        for label in labels:\n",
    "            if(label in [\"Name calling/Labeling\" , \"Doubt\" , \"Smears\" , \"Reductio ad hitlerum\"]):\n",
    "                ground_truth[label] = 1\n",
    "                ground_truth[\"Ethos\"] = 1\n",
    "                ground_truth[\"Ad Hominem\"] = 1\n",
    "            \n",
    "            if(label in [\"Bandwagon\",\"Appeal to authority\"]):\n",
    "                ground_truth[label] = 1\n",
    "                ground_truth[\"Ethos\"] = 1\n",
    "                ground_truth[\"Logos\"] = 1\n",
    "                ground_truth[\"Justification\"] = 1\n",
    "            \n",
    "            if(label in [\"Glittering generalities (Virtue)\"]):\n",
    "                ground_truth[label] = 1\n",
    "                ground_truth[\"Ethos\"] = 1\n",
    "            \n",
    "            if(label in [\"Transfer\"]):\n",
    "                ground_truth[label] = 1\n",
    "                ground_truth[\"Ethos\"] = 1\n",
    "                ground_truth[\"Pathos\"] = 1\n",
    "            \n",
    "            if(label in [\"Appeal to (Strong) Emotions\",\"Exaggeration/Minimisation\",\"Loaded Language\"]):\n",
    "                ground_truth[label] = 1\n",
    "                ground_truth[\"Pathos\"] = 1\n",
    "            \n",
    "            if(label in [\"Flag-waving\",\"Appeal to fear/prejudice\"]):\n",
    "                ground_truth[label]=1\n",
    "                ground_truth[\"Pathos\"] = 1\n",
    "                ground_truth[\"Logos\"] = 1\n",
    "                ground_truth[\"Justification\"] = 1\n",
    "            \n",
    "            if(label in [\"Slogans\"]):\n",
    "                ground_truth[label] = 1\n",
    "                ground_truth[\"Justification\"] = 1\n",
    "                ground_truth[\"Logos\"] = 1\n",
    "            \n",
    "            if(label in [\"Repetition\",\"Obfuscation, Intentional vagueness, Confusion\"]):\n",
    "                ground_truth[label] = 1\n",
    "                ground_truth[\"Logos\"] = 1\n",
    "            \n",
    "            if(label in [\"Misrepresentation of Someone's Position (Straw Man)\",\"Presenting Irrelevant Data (Red Herring)\"]):\n",
    "                ground_truth[label] = 1\n",
    "                ground_truth[\"Logos\"] = 1\n",
    "                ground_truth[\"Distraction\"] = 1\n",
    "                ground_truth[\"Reasoning\"] = 1\n",
    "            \n",
    "            if(label in [\"Whataboutism\"]):\n",
    "                ground_truth[label] = 1\n",
    "                ground_truth[\"Ethos\"] = 1\n",
    "                ground_truth[\"Ad Hominem\"] = 1\n",
    "                ground_truth[\"Logos\"] = 1\n",
    "                ground_truth[\"Distraction\"] = 1\n",
    "                ground_truth[\"Reasoning\"] = 1\n",
    "            \n",
    "            if(label in [\"Causal Oversimplification\", \"Black-and-white Fallacy/Dictatorship\", \"Thought-terminating cliché\"]):\n",
    "                ground_truth[label] = 1\n",
    "                ground_truth[\"Logos\"] = 1\n",
    "                ground_truth[\"Reasoning\"] = 1\n",
    "                ground_truth[\"Simplification\"] = 1\n",
    "\n",
    "        gt = list(ground_truth.keys())\n",
    "        return gt\n",
    "    \n",
    "    def read_json_data(self,file_path):\n",
    "        f = open(file_path)\n",
    "        train_json_data = json.load(f)\n",
    "        f.close()\n",
    "        return train_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4cd12-8b65-42cd-9d65-a82d5d7876e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = customDataset(train_json_path,train_images_folder_path)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add101a-c532-4f5f-9057-ed92a5644c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Processor(nn.Module):\n",
    "    def __init__(self , device):\n",
    "        super(Text_Processor, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 1000 , device=device)  \n",
    "        self.fc2 = nn.Linear(1000, 500 , device=device)\n",
    "        self.dp = nn.Dropout(p=0.2)\n",
    "   \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dp(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class Image_Processor(nn.Module):\n",
    "    def __init__(self , device):\n",
    "        super(Image_Processor, self).__init__()\n",
    "        self.fc1 = nn.Linear(1000, 1000 , device=device)  \n",
    "        self.fc2 = nn.Linear(1000, 500 , device=device)\n",
    "        self.dp = nn.Dropout(p=0.2)\n",
    "   \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dp(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class Text_Image_Processor(nn.Module):\n",
    "    def __init__(self , device):\n",
    "        super(Text_Image_Processor, self).__init__()\n",
    "        self.text_processor = Text_Processor(device)\n",
    "        self.image_processor = Image_Processor(device)\n",
    "        self.fc1 = nn.Linear(1000, 500 , device=device)\n",
    "        self.fc2 = nn.Linear(500, 250 , device=device)\n",
    "        self.fc3 = nn.Linear(250, 250 , device=device)\n",
    "        self.fc4 = nn.Linear(250,30 , device=device)\n",
    "        self.dp = nn.Dropout(p=0.2)\n",
    "   \n",
    "    def forward(self,image_feature , text_feature):\n",
    "        i_f = self.image_processor(image_feature)\n",
    "        t_f = self.text_processor(text_feature)\n",
    "        c_f = torch.concat((i_f,t_f))\n",
    "        x = F.relu(self.fc1(c_f))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dp(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f85c20-fb92-4903-a47e-2a24515310e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI_P = Text_Image_Processor(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4119401-3a55-41ba-916b-39000540d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf221319-bde9-41da-a1db-02b9a583064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_features,  text_features ,  train_labels = next(iter(train_dataloader))\n",
    "# for image_features, text_features , train_labels in enumerate(next(iter(train_dataloader))):\n",
    "#     pass\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(TI_P.parameters(), lr=0.001)\n",
    "\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321e1c5-feef-4bbf-b6cc-78c21b7bce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI_P.load_state_dict(torch.load(\"./model_v3.pt\"))\n",
    "TI_P.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c96ea8-f222-4626-8f9b-58489813e1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    try:\n",
    "        image_batch,text_batch,target_batch =  next(iter(train_dataloader))\n",
    "    except:\n",
    "        print(\"Missed an epoch\")\n",
    "        continue\n",
    "    for input_image,input_text, target in zip(image_batch,text_batch,target_batch):\n",
    "        try:    \n",
    "            # i_f , t_f , gt = train_dataset[x]\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = TI_P(input_image,input_text)\n",
    "            # print(output)\n",
    "            # print(target)\n",
    "            loss = criterion(output.float().to(\"cpu\"), target.float().to(\"cpu\"))\n",
    "        \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # if x % 10 == 9:\n",
    "            #     print(f\"[{epoch + 1}, {x + 1}] loss: {running_loss / 10:.3f}\")\n",
    "            #     running_loss = 0.0\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"Epoch # {epoch} has average running loss = {running_loss/500}\")\n",
    "    if(epoch%20==0):\n",
    "        torch.save(TI_P.state_dict(), \"./model_v3.pt\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2f148-9d11-44bb-824a-9c8e9ce3963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(TI_P.state_dict(), \"./model_v3.pt\")\n",
    "# faults = []\n",
    "# for x in range(0,len(train_dataset)):\n",
    "#     try:\n",
    "#         a,b,c = train_dataset[x]\n",
    "#     except:\n",
    "#         print(\"error\",x)\n",
    "#         faults.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65374f86-5ee7-41e1-bf31-3f2c1fdc6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.train_json_data[163]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8af74-be8d-49f8-9041-d264c03573d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_targets = [\n",
    "#     \"Logos\",\n",
    "#     \"Repetition\",\n",
    "#     \"Obfuscation, Intentional vagueness, Confusion\",\n",
    "#     \"Reasoning\",\n",
    "#     \"Justification\",\n",
    "#     \"Slogans\",\n",
    "#     \"Bandwagon\",\n",
    "#     \"Appeal to authority\",\n",
    "#     \"Flag-waving\",\n",
    "#     \"Appeal to fear/prejudice\",\n",
    "#     \"Simplification\",\n",
    "#     \"Causal Oversimplification\",\n",
    "#     \"Black-and-white Fallacy/Dictatorship\",\n",
    "#     \"Thought-terminating cliché\",\n",
    "#     \"Distraction\",\n",
    "#     \"Misrepresentation of Someone's Position (Straw Man)\",\n",
    "#     \"Presenting Irrelevant Data (Red Herring)\",\n",
    "#     \"Whataboutism\",\n",
    "#     \"Ethos\",\n",
    "#     \"Glittering generalities (Virtue)\",\n",
    "#     \"Ad Hominem\",\n",
    "#     \"Doubt\",\n",
    "#     \"Name calling/Labeling\",\n",
    "#     \"Smears\",\n",
    "#     \"Reductio ad hitlerum\",\n",
    "#     \"Pathos\",\n",
    "#     \"Exaggeration/Minimisation\"\n",
    "#     \"Loaded Language\",\n",
    "#     \"Transfer\",\n",
    "#     \"Appeal to (Strong) Emotions\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4955cd-7233-4a8d-b1b2-21c483202dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels =  [\n",
    "#             \"Smears\",\n",
    "#             \"Misrepresentation of Someone's Position (Straw Man)\"\n",
    "#         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989a73c-a89a-4646-a991-819dff91a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth = {}\n",
    "# for label in labels:\n",
    "#     if(label in [\"Name calling/Labeling\" , \"Doubt\" , \"Smears\" , \"Reductio ad hitlerum\"]):\n",
    "#         ground_truth[label] = 1\n",
    "#         ground_truth[\"Ethos\"] = 1\n",
    "#         ground_truth[\"Ad Hominem\"] = 1\n",
    "\n",
    "#     if(label in [\"Bandwagon\",\"Appeal to authority\"]):\n",
    "#         ground_truth[label] = 1\n",
    "#         ground_truth[\"Ethos\"] = 1\n",
    "#         ground_truth[\"Logos\"] = 1\n",
    "#         ground_truth[\"Justification\"] = 1\n",
    "\n",
    "#     if(label in [\"Glittering generalities (Virtue)\"]):\n",
    "#         ground_truth[label] = 1\n",
    "#         ground_truth[\"Ethos\"] = 1\n",
    "\n",
    "#     if(label in [\"Transfer\"]):\n",
    "#         ground_truth[label] = 1\n",
    "#         ground_truth[\"Ethos\"] = 1\n",
    "#         ground_truth[\"Pathos\"] = 1\n",
    "\n",
    "#     if(label in [\"Appeal to (Strong) Emotions\",\"Exaggeration/Minimisation\",\"Loaded Language\"]):\n",
    "#         ground_truth[label] = 1\n",
    "#         ground_truth[\"Pathos\"] = 1\n",
    "\n",
    "#     if(label in [\"Flag-waving\",\"Appeal to fear/prejudice\"]):\n",
    "#         ground_truth[label]=1\n",
    "#         ground_truth[\"Pathos\"] = 1\n",
    "#         ground_truth[\"Logos\"] = 1\n",
    "#         ground_truth[\"Justification\"] = 1\n",
    "\n",
    "#     if(label in [\"Slogans\"]):\n",
    "#         ground_truth[label] = 1\n",
    "#         ground_truth[\"Justification\"] = 1\n",
    "#         ground_truth[\"Logos\"] = 1\n",
    "\n",
    "#     if(label in [\"Repetition\",\"Obfuscation, Intentional vagueness, Confusion\"]):\n",
    "#         ground_truth[label] = 1\n",
    "#         ground_truth[\"Logos\"] = 1\n",
    "\n",
    "#     if(label in [\"Misrepresentation of Someone's Position (Straw Man)\",\"Presenting Irrelevant Data (Red Herring)\"]):\n",
    "#         ground_truth[label] = 1\n",
    "#         ground_truth[\"Logos\"] = 1\n",
    "#         ground_truth[\"Distraction\"] = 1\n",
    "#         ground_truth[\"Reasoning\"] = 1\n",
    "\n",
    "#     if(label in [\"Whataboutism\"]):\n",
    "#         ground_truth[label] = 1\n",
    "#         ground_truth[\"Ethos\"] = 1\n",
    "#         ground_truth[\"Ad Hominem\"] = 1\n",
    "#         ground_truth[\"Logos\"] = 1\n",
    "#         ground_truth[\"Distraction\"] = 1\n",
    "#         ground_truth[\"Reasoning\"] = 1\n",
    "\n",
    "#     if(label in [\"Causal Oversimplification\", \"Black-and-white Fallacy/Dictatorship\", \"Thought-terminating cliché\"]):\n",
    "#         ground_truth[label] = 1\n",
    "#         ground_truth[\"Logos\"] = 1\n",
    "#         ground_truth[\"Reasoning\"] = 1\n",
    "#         ground_truth[\"Simplification\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829bd1a6-ac9a-404e-861f-3b77cd9472d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt = list(ground_truth.keys())\n",
    "# gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead0511-d098-47ed-98b4-8effb55b9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Define your categories\n",
    "# categories = all_targets\n",
    "# num_categories = len(categories)\n",
    "\n",
    "# # Define your ground truth for multi-label classification\n",
    "# multi_label_ground_truth = [gt]\n",
    "\n",
    "# # Create a tensor for multi-label classification\n",
    "# tensor = np.zeros((len(multi_label_ground_truth), num_categories))\n",
    "# for i, labels in enumerate(multi_label_ground_truth):\n",
    "#     indices = [categories.index(label) for label in labels]\n",
    "#     tensor[i, indices] = 1\n",
    "\n",
    "# print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd1245-7c52-4a4b-9117-7f3097b86b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e3a35-b4a9-49ea-81e4-fd0144d351d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
