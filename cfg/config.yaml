dataset:
  root: 'dataset'  # not needed if using pre-extracted bottom-up features

text-model:
  name: 'roberta'
  pretrain: 'cardiffnlp/twitter-roberta-base-sentiment-latest'
  word-dim: 768
  extraction-hidden-layer: 10
  fine-tune: False

image-model:
  name: 'google/vit-base-patch16-224'
  enabled: True
  fine-tune: False
  feat-dim: 2048
  grid: [7, 7]

model:
  name: 'transformer'
  embed-dim: 1024
  feedforward-dim: 1024
  num-layers: 4

training:
  balanced-sampling: False
  lr: 0.00005  # 0.000006
  pretrained-modules-lr: 0.000001  # learning rate for early pretrained layers
  grad-clip: 2.0
  bs: 8
  scheduler: 'steplr'
  milestones: [40]
  gamma: 0.1